{"cells":[{"cell_type":"markdown","metadata":{"id":"mbZXQ3rA3NwL"},"source":["# **Lab 6: Introduction to Natural Language Processing (NLP)**\n","---\n","\n","### **Description**\n","In today's lab, we will see how to use neural networks for one of the most popular NLP tasks: **text classification**. This will involve applying what you already know about neural nets and new NLP concepts of tokenization and vectorization.\n","\n","For this project, we will be working with the `fetch_20newsgroups` dataset, which is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. Each newsgroup covers a different topic, such as sports, politics, religion, and technology. The documents within each newsgroup were posted by various authors, and cover a wide range of subtopics related to the main theme of the newsgroup.\n","\n","The goal of this project is to build a machine learning model that can accurately classify newsgroup documents based on their content.\n","\n","<br>\n","\n","### **Lab Structure**\n","**Part 1**: [Tokenization and Vectorization](#p1)\n","\n","**Part 2**: [News Group Classification with a Neural Network](#p2)\n",">\n",">**Part 2.1**: [Tokenizing and Vectorizing the News Groups Dataset](#p2.1)\n",">\n",">**Part 2.2**: [Training and Testing a Neural Network](#p2.2)\n","\n","**Part 3**: [News Group Classification with a CNN](#p3)\n","\n","\n","\n","<br>\n","\n","### **Goals**\n","By the end of this lab, you will:\n","* Understand the concept of tokenization in NLP.\n","* Compare a fully connected network to a CNN for text classification.\n","\n","<br>\n","\n","### **Cheat Sheets**\n","[Natural Language Processing I](https://docs.google.com/document/d/1MamYMxe8zlWoiDc0tX2RzUKQULCPVUh-2QtdzRRvzcs/edit?usp=sharing)\n","\n","<br>\n","\n","**Before starting, run the code below to import all necessary functions and libraries.**\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"YAvvLhRIoqYp"},"outputs":[],"source":["from sklearn.datasets import fetch_20newsgroups\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from fastai.text.all import *\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"_X9DchFl6uZg"},"source":["<a name=\"p1\"></a>\n","\n","---\n","## **Part 1: Tokenization and Vectorization**\n","---\n","\n","**Run the cell below to load a simple corpus for us to work with.**"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"jxb7Iynu6q2a"},"outputs":[],"source":["# Define a collection of text documents\n","corpus = [\n","    \"This is the first document.\",\n","    \"This is the second document.\",\n","    \"And this is the third document.\",\n","    \"Is this the first document?\",\n","]"]},{"cell_type":"markdown","metadata":{"id":"ZZPhj3RW68Kq"},"source":["#### **Problem #1.1: Create a CountVectorizer object**\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"xtKR2kbk8zpw"},"outputs":[],"source":["vectorizer = CountVectorizer()"]},{"cell_type":"markdown","metadata":{"id":"Yt03nQ5Z7D2u"},"source":["#### **Problem #1.2: Fit the vectorizer to the corpus**\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ISVa9A52832F"},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: black;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-1 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-1 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-1 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-1 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-1 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-1 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-1 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-1 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: block;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"▸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-1 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"▾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-1 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-1 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-1 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-1 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-1 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 1ex;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-1 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-1 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;CountVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">?<span>Documentation for CountVectorizer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>CountVectorizer()</pre></div> </div></div></div></div>"],"text/plain":["CountVectorizer()"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["vectorizer.fit(corpus)"]},{"cell_type":"markdown","metadata":{"id":"PSVTgBD67QFQ"},"source":["#### **Problem #1.3: Transform the corpus into a matrix of token counts**\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"mxfzW9c484nz"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0 1 1 1 0 1 0 1]\n"," [0 1 0 1 1 1 0 1]\n"," [1 1 0 1 0 1 1 1]\n"," [0 1 1 1 0 1 0 1]]\n"]}],"source":["# Transform the corpus into a matrix of token counts\n","X = vectorizer.transform(corpus)\n","\n","# Print the resulting matrix\n","print(X.toarray())"]},{"cell_type":"markdown","metadata":{"id":"K7jj8YPZ7hFR"},"source":["#### **Problem #1.4: Print the tokens**\n","\n","Use `get_feature_names_out()` to print the tokens.\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"11eK9s-y86TR"},"outputs":[{"name":"stdout","output_type":"stream","text":["['and' 'document' 'first' 'is' 'second' 'the' 'third' 'this']\n"]}],"source":["get_feature_names = vectorizer.get_feature_names_out()\n","print(get_feature_names)"]},{"cell_type":"markdown","metadata":{"id":"4d_cmQjP7xKT"},"source":["Compare the tokens, the matrix, and the corpus. Do you see how each sentence is represented in the matrix?"]},{"cell_type":"markdown","metadata":{"id":"kV7du9rJBYuz"},"source":["<a name=\"p2\"></a>\n","\n","---\n","## **Part 2: News Group Classification with a Neural Network**\n","---\n","\n","\n","The 20 Newsgroups dataset is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. Our task is to classify the articles to the correct newsgroup.\n","\n","<br>\n","\n","**Run the cell below to load the dataset.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"foasGD-f9a_R"},"outputs":[],"source":["# Load the dataset\n","newsgroups_data = fetch_20newsgroups(\n","    subset='train',\n","    remove=('headers', 'footers', 'quotes')\n",")\n","\n","texts = newsgroups_data.data\n","labels = newsgroups_data.target\n","\n","# Split the dataset into training and validation sets\n","texts_train, texts_val, labels_train, labels_val = train_test_split(\n","    texts,\n","    labels,\n","    test_size=0.2,\n","    random_state=42\n",")"]},{"cell_type":"markdown","metadata":{"id":"ZqaEmJy38VkC"},"source":["<a name=\"p2.1\"></a>\n","\n","---\n","### **Part 2.1: Tokenizing and Vectorizing the News Groups Dataset**\n","---"]},{"cell_type":"markdown","metadata":{"id":"imQT4CJS9nxb"},"source":["#### **Problem #2.1.1: Create the CountVectorizer object**\n","\n","Initialize the vectorizer with the following parameters:\n","* `stop_words='english'`\n","* `max_features=4000`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WzdtPvyx-ew7"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"G2xgieYx-BIy"},"source":["#### **Problem #2.1.2: Fit and transform the training data.**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-AAplhm8-fQa"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"zuJwVtiL-G0y"},"source":["#### **Problem #2.1.3: Transform the validation data.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kmk0hhfd-dbz"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"KPeFKyUx_RX4"},"source":["###### **Run the code below to print out the shapes of each BoW matrix and a sample of the vocabulary.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLcL6cn5JU0t"},"outputs":[],"source":["# Show the shape of the BoW matrices\n","print(\"Shape of the training BoW matrix:\", X_train_bow.shape)\n","print(\"Shape of the validation BoW matrix:\", X_valid_bow.shape)\n","\n","print(L(vectorizer.get_feature_names_out()[2000:2100]))"]},{"cell_type":"markdown","metadata":{"id":"_iLEJ5_vAUzH"},"source":["#### **Problem #2.1.4: Choose a random document and print out its BoW representation.**\n","\n","Then use `get_feature_names_out()` to determine what some of the words are."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XOm2QWcxAtEg"},"outputs":[],"source":["random_doc_idx = # You can choose any index\n","print(\"BoW representation of a random document:\\n\", X_train_bow[random_doc_idx])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVuwXMMLAtgH"},"outputs":[],"source":["# Use get_feature_names_out() to explore your results"]},{"cell_type":"markdown","metadata":{"id":"XGMs3RKZI8Of"},"source":["<a name=\"p2.2\"></a>\n","\n","---\n","### **Part 2.2: Training and Testing a Neural Network**\n","---\n","\n","At this point, we have imported, split, and vectorized the data. Now we need to prepare it for a PyTorch model and proceed as we would for *any* classification task with a PyTorch model."]},{"cell_type":"markdown","metadata":{"id":"eN8Z-C_3K0ZG"},"source":["#### **Step #1**\n","\n","**This code has been provided for you. Run the cell below.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"liLaJe3dLPN9"},"outputs":[],"source":["# Convert to PyTorch tensors\n","X_train = torch.tensor(X_train_bow.todense()).float()\n","X_valid = torch.tensor(X_valid_bow.todense()).float()\n","\n","# Extract labels\n","y_train = torch.tensor(labels_train)\n","y_valid = torch.tensor(labels_val)\n","\n","# Create DataLoaders\n","train_dataset = list(zip(X_train, y_train))\n","valid_dataset = list(zip(X_valid, y_valid))\n","\n","train_dl = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_dl = DataLoader(valid_dataset, batch_size=64)\n","dls = DataLoaders(train_dl,val_dl)"]},{"cell_type":"markdown","metadata":{"id":"hc5z8AhGCKHt"},"source":["#### **Step #2**\n","\n","For a fully connected network, the dimension of the input layer will be the number of tokens. Complete the code below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4H2ukzNCj41"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ZbhgBwEPCm8P"},"source":["#### **Steps #3-6**\n","\n","Define a fully connected network of your own design. Ensure you have the correct number of inputs and outputs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qrIklOq1CmS0"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"fuGPjq1bLSNA"},"source":["#### **Step #7**\n","\n","Create a Learner object and fit the model. Since this is a multiclass classification problem, you will use `nn.CrossEntropyLoss()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rjiBv6JzLk4C"},"outputs":[],"source":["# Create a Learner and train the model\n"]},{"cell_type":"markdown","metadata":{"id":"JSOOynZqL0xo"},"source":["#### **Step #8**\n","\n","Now, evaluate the model for both the training and validation sets.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HqPSJlsSL7_E"},"outputs":[],"source":["# Evaluate the training set\n","\n","\n","# Evaluate the test set\n"]},{"cell_type":"markdown","metadata":{"id":"zkn0RBS0DHPo"},"source":["#### **How did your model perform?**"]},{"cell_type":"markdown","metadata":{"id":"r4_U4kWylKLw"},"source":["<a name=\"p3\"></a>\n","\n","---\n","## **Part 3: News Group Classification with a CNN**\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BQ6jclf5Po-o"},"source":["#### **Step #1**\n","\n","**The code for importing the data is provided for you. Run the cell below.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LY31FfYdP3xa"},"outputs":[],"source":["# Convert to PyTorch tensors\n","X_train = torch.tensor(X_train_bow.todense()).float().unsqueeze(1)\n","X_valid = torch.tensor(X_valid_bow.todense()).float().unsqueeze(1)\n","\n","# Extract labels\n","y_train = torch.tensor(labels_train)\n","y_valid = torch.tensor(labels_val)\n","\n","# Create DataLoaders\n","train_dataset = list(zip(X_train, y_train))\n","valid_dataset = list(zip(X_valid, y_valid))\n","\n","train_dl = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_dl = DataLoader(valid_dataset, batch_size=64)\n","dls = DataLoaders(train_dl,val_dl)\n","\n","input_dims = len(vectorizer.get_feature_names_out())"]},{"cell_type":"markdown","metadata":{"id":"1AAVJHkuE3nM"},"source":["#### **Steps #3-6**\n","\n","Let's start by building a new CNN model. Remember, the syntax for CNNs for NLP is a little different than for images. We will be using the 1D versions of the convolution and max pooling layers. Examples:\n","* `nn.Conv1d(64, 128, kernel_size=5, padding=2)`\n","* `nn.MaxPool1d(2)`\n","\n","Define a CNN with the following layers:\n","\n","Block 1:\n","* A convolutional layer with the appropriate input dimension and 16 outputs, kernel size of 3, `padding=1`, and ReLU activation.\n","* A max pooling layer with a pool size of 2\n","\n","Block 2\n","* A convolutional layer with 32 outputs, kernel size of 3, `padding=1`, and ReLU activation.\n","* A max pooling layer with a pool size of 2\n","\n","Finally, add:\n","* A linear layer with 8 outputs and ReLU activation\n","* The output layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YVE6HlQ4E3nc"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"M__-6lOuE3nc"},"source":["#### **Step #7**\n","\n","Create a Learner object and fit the model. Since this is a multiclass classification problem, you will use `nn.CrossEntropyLoss()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OaB29Xv2E3nd"},"outputs":[],"source":["# Create a Learner and train the model\n"]},{"cell_type":"markdown","metadata":{"id":"r5KifHJwE3ne"},"source":["#### **Step #8**\n","\n","Now, evaluate the model for both the training and validation sets.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k_8YoV92E3ne"},"outputs":[],"source":["# Evaluate the training set\n","\n","\n","# Evaluate the test set\n"]},{"cell_type":"markdown","metadata":{"id":"S6ddmcs4xIKQ"},"source":["**Oh no!** It looks like the CNN didn't do much better! It turns out that tokenization and vectorization is not enough to prepare text data for deep learning. There's an additional processing step we can take that will set our models up for success: **embedding.** We will see how embedding improves model performance in the next lab."]},{"cell_type":"markdown","metadata":{"id":"7dzC09dLlEhm"},"source":["# End of notebook\n","---\n","© 2024 The Coding School, All rights reserved"]}],"metadata":{"colab":{"collapsed_sections":["7dzC09dLlEhm"],"provenance":[{"file_id":"1PupwQQp6LZFfMIPgWLKlWXrYDgLuA5sj","timestamp":1678127175525}],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
